{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all data was scraped from:\n",
    "https://www.worldfootball.net\n",
    "\n",
    "## Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import lxml\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the number of players to embedded in each match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_players = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scraping a single match\n",
    "\n",
    "In scrap_match method we are requstion a single match data from worldfootball.net, which include the match score, home team and away team number of players we requested (using the starting_lineup method).\n",
    "\n",
    "Since some sources pages have different data, we are making sure we are indeed scraping the relevant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def starting_lineup(table_players,players_list):\n",
    "    counter = 0\n",
    "    for i in table_players.contents:\n",
    "        if (len(i) > 4) & (counter <number_players):\n",
    "            players_list.append(i.contents[3].contents[1]['title'])\n",
    "            counter +=1\n",
    "    \n",
    "def scrape_match(season,team_home,team_away, debug=False):\n",
    "    html = 'https://www.worldfootball.net/report/premier-league-' + season + '-' + team_home + '-' +team_away\n",
    "    if debug:\n",
    "        print(html)\n",
    "    if html == \"https://www.worldfootball.net/report/premier-league-2017-2018-southampton-fc-west-ham-united\":\n",
    "        html = html + \"_2\"\n",
    "    source = requests.get(html).text\n",
    "    soup = BeautifulSoup(source,'lxml')\n",
    "    score = soup.find_all(class_='standard_tabelle')[0]\n",
    "    table_players_home = soup.find_all(class_='standard_tabelle')[2]\n",
    "    if len(table_players_home.contents) > 10:\n",
    "        table_players_away = soup.find_all(class_='standard_tabelle')[3]\n",
    "    else:\n",
    "        table_players_home = soup.find_all(class_='standard_tabelle')[3]\n",
    "        table_players_away = soup.find_all(class_='standard_tabelle')[4]\n",
    "    players_list = []\n",
    "    starting_lineup(table_players_home,players_list)\n",
    "    starting_lineup(table_players_away,players_list)\n",
    "    text=score.contents[3].contents[3].contents[1].text\n",
    "    score_arr = []\n",
    "    for c in text: \n",
    "        if c.isdigit():\n",
    "            score_arr.append(int(c))\n",
    "    return players_list,score_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping a season\n",
    "\n",
    "In the scrape_season method we are scarping an entire season.\n",
    "after requesing a page with all the leauge teams in the relevant season, we are creating an array of all the combinations in that season.\n",
    "\n",
    "Then we create a fixtures DataFrame which will hold every match starting lineup and score, and players_df DataFrame which hold every player preformence.\n",
    "\n",
    "We call every possible match with our scrape_match method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_season(season, debug=False):\n",
    "    seaon_source = str(season - 1) + '-' + str(season)\n",
    "    source_team = requests.get('https://www.worldfootball.net/players/eng-premier-league-'+seaon_source +'/').text\n",
    "    soup_team = BeautifulSoup(source_team,'lxml')\n",
    "    table_teams = soup_team.find_all(class_='standard_tabelle')[0]\n",
    "    teams_list = []\n",
    "    for i in table_teams.contents:\n",
    "        if len(i) > 1:\n",
    "            teams_list.append(i.contents[1].contents[1]['href'].split('teams/')[1].split('/')[0])\n",
    "    fixtures_columns = ['home_team','away_team']\n",
    "    for which_team in ['home', 'away']:\n",
    "        for i in range(1,number_players+1):\n",
    "            fixtures_columns.append(which_team +(str(i)))\n",
    "    fixtures_columns.append('home_score')\n",
    "    fixtures_columns.append('away_score')\n",
    "    fixtures = pd.DataFrame(columns=fixtures_columns)\n",
    "    players_columns = ['name','team']\n",
    "    players_df = pd.DataFrame(columns=players_columns)\n",
    "    every_fixture = []\n",
    "    for team_1 in teams_list:\n",
    "        for team_2 in teams_list:\n",
    "            if team_1 != team_2:\n",
    "                every_fixture.append([team_1,team_2])\n",
    "    for fixture in every_fixture:\n",
    "        players_list,score_arr = scrape_match(seaon_source,fixture[0],fixture[1],debug)\n",
    "        fixtures = fixtures.append(pd.Series(fixture +players_list +score_arr,index=fixtures.columns),ignore_index=True)\n",
    "        players_df_current = pd.DataFrame(columns=players_columns)\n",
    "        players_df_current['name'] = players_list\n",
    "        players_df_current['team'] = [fixture[0]]*number_players+[fixture[1]]*number_players\n",
    "        players_df = pd.concat([players_df,players_df_current])\n",
    "    \n",
    "    fixtures['season'] = str(season)\n",
    "    players_df['season'] = str(season)\n",
    "    players_df['count']=1\n",
    "    \n",
    "    return fixtures,players_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting our data\n",
    "\n",
    "Finally we call each season and collect data using the scrape_season method.\n",
    "\n",
    "each season fixtures and player_df is saved as a csv.\n",
    "\n",
    "We concat all the season data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the time for 2015 season is 493.22929191589355\n",
      "the time for 2016 season is 517.2812554836273\n",
      "the time for 2017 season is 506.21851801872253\n",
      "the time for 2018 season is 494.5997040271759\n",
      "the time for 2019 season is 503.86867809295654\n"
     ]
    }
   ],
   "source": [
    "seasons = [2015,2016,2017,2018,2019]\n",
    "seasons_dict = {}\n",
    "for season in seasons:    \n",
    "    start = time.time()\n",
    "    seasons_dict[str(season)] = {}\n",
    "    seasons_dict[str(season)]['fixtures'],seasons_dict[str(season)]['players_df'] = scrape_season(season)\n",
    "    seasons_dict[str(season)]['fixtures'].to_csv('fixtures_' +str(season)+'.csv',index=False,encoding='utf-8')\n",
    "    seasons_dict[str(season)]['players_df'].to_csv('players_df_'+str(season)+'.csv',index=False,encoding='utf-8')\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    print(\"the time for \" + str(season) + \" season is \" + str(elapsed))\n",
    "\n",
    "all_fixtures = pd.concat([seasons_dict[str(season)]['fixtures'] for season in seasons])\n",
    "all_players = pd.concat([seasons_dict[str(season)]['players_df'] for season in seasons])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In odrer to create an embedding of all the players, we are giving each player an ID.\n",
    "\n",
    "We merge by name in order to give each player an unique ID and merge it with the season and team citeria in order to merge it later between the fixtures and player ID "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17.294166321886635, 17.0, 11.404835471538657)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_players_groupby = all_players.groupby(['name','team','season']).count().reset_index()\n",
    "np.mean(all_players_groupby['count']),np.median(all_players_groupby['count']),np.std(all_players_groupby['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_season_names = all_players.groupby(['name']).sum().reset_index()\n",
    "all_season_names['id'] = range(1,len(all_season_names)+1)\n",
    "all_players_groupby=all_players_groupby.merge(all_season_names[['name','id']],on='name',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_players_groupby.to_csv('all_players_groupby_2015_2019.csv',index=False,encoding='utf-8')\n",
    "all_fixtures.to_csv('all_fixtures_2015_2019.csv',index=False,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
